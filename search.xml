<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>test.md</title>
      <link href="/2018/04/29/test/"/>
      <url>/2018/04/29/test/</url>
      <content type="html"><![CDATA[<h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><h3 id="test1"><a href="#test1" class="headerlink" title="test1"></a>test1</h3><p>$x^2 = y$</p><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>1、单层RNN模型</p><p><img src="http://owxv9mpb2.bkt.clouddn.com/base_rnn.jpg" alt="base rnn"></p><ol><li>上图是一个最简单的单层RNN模型，其中每一个节点相当于原来的全连接网络，<code>$x_t, x_{t+1}$</code>就是时间序列的输入，每一项是原来的单独的一个输入（也是n维的）。</li><li>计算<ol><li>$O_t = g(V_{S_t}) $</li><li>$V_{S_t} = f(U x_t + S_{t-1} w)$<code></code>$S_{t-1}$是上一个节点的输出，w为权值矩阵，f是激活函数 </li><li>从上图可以看出同一层的RNN权值是共享的</li></ol></li><li>总结<ol><li><del>RNN的参数更多</del>虽然参数多，但是权值是共享的啊<ol><li>存在多个全连接的节点</li><li>不仅有全连接的参数，还多了上一层往下一层传递的参数</li></ol></li><li>将公式依次带入，发现当前的输出会受之前的所有输入的影响</li></ol></li></ol><p>2、双向RNN模型</p><p><img src="http://owxv9mpb2.bkt.clouddn.com/bi_rnn.png" alt="bi rnn"></p><ol><li>为啥要考虑双向：有时候，不仅仅需要某个节点前面的信息，也需要节点后面的信息，而基本的RNN无法得到节点后面的信息；</li><li>如上图所示，A就相当于单层RNN中S，属于中间状态，注意此处不同层之间的权值是不共享的<ol><li>$y_2 = g(V^- A_2^- + V A_2)$</li><li>$A_2 = f(Ux_2 + W A_1)$</li><li>$A_2^- = f(U^-x_2 + W^- A_1^-)$</li></ol></li></ol><p>3、多层RNN模型</p><p><img src="http://owxv9mpb2.bkt.clouddn.com/multi_rnn.png" alt="multi rnn"></p><ol><li>第i层隐藏层的输入是第i-1层隐藏层输入和上一个节点输出的共同输出；</li></ol><p>4、RNN的训练算法：BPTT</p><p><img src="http://owxv9mpb2.bkt.clouddn.com/rnn_grad.jpg" alt="rnn_grad"></p><ol><li>符号定义<ul><li>$O_t = \varphi(VS_t)$</li><li>$S_t = \phi(WS_{t-1}+UX_t)$</li><li>令$S^<em><em>t = WS</em>{t-1}+UX_t$<code>和</code>$O^</em>_t = VS_t$</li></ul></li><li><p>​</p><ol><li><a href="https://zhuanlan.zhihu.com/p/26892413" target="_blank" rel="noopener">详细梯度推导</a></li><li>总结<ul><li>这些推导虽然极其复杂且不知道为什么要这么求导</li><li>将复杂的求导根据链式法则转换到关键的变量之间的求导上，比如<code>$S_k^*$</code>对<code>$S_{k-1}^*$</code>的求导</li></ul></li></ol></li><li><p>梯度爆炸和梯度消失</p><ul><li>从iv的第二个式子可以看出来，对于比较之前的梯度，因为有乘积的形式，所以会导致最后的梯度是指数形式，如果指数形式内部是小于1或者大于1，很容易就出现梯度消失和爆炸的问题</li><li>解决<ul><li>梯度消失<ul><li>改用别的激活函数，如ReLU，但是容易发生梯度爆炸，需要初始化参数时尽量小</li><li>修改state的传递方式</li></ul></li></ul></li></ul></li></ol><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p><img src="http://owxv9mpb2.bkt.clouddn.com/lstm.png" alt="lstm"></p><ol><li>引出<ol><li>RNN的梯度是一个指数函数，当序列比较长的时候，容易出现梯度消失和梯度爆炸的问题；</li><li>梯度对短期的数据比较敏感，因此可以考虑加入长期的记忆c</li></ol></li><li>前向计算<ol><li>遗忘门<ol><li>控制上个单元的状态<code>$c_{t-1}$</code>有多少进入到当前的单元状态中</li><li><code>$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$</code></li><li><code>$W_f [h_{t-1},x_t] = w_{fh}h_{t-1} + w_{fx}x_t$</code></li><li>根据当前的输入和上一个节点的输出决定对前一个节点的状态单元保留多少（长期记忆）</li></ol></li><li>输入门<ol><li>此刻的状态有多少进入到当前的单元状态（长期记忆）</li><li><code>$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$</code></li><li>根据当前的输入和上一个节点的输出决定对当前状态输出多少到当前节点的状态单元</li></ol></li><li>当前节点本应该输出的状态<code>$c_t^-$</code><ol><li>当前节点本应该输出的节点状态</li><li><code>$c_t^- = tanh(W_c[h_{t-1}, x_t] + b_c)$</code></li></ol></li><li>当前节点的单元状态<ol><li>根据该节点本应该输出的状态和输入门的阈值加上遗忘门控制的<code>$c_{t-1}$</code>得到当前节点的单元状态</li><li><code>$c_t = i_t . c_t^- + c_{t-1}.f_t$</code></li></ol></li><li>输出门<ol><li>得到将单元状态输出多少的阈值矩阵</li><li><code>$o_t = W_o[h_{t-1}, x_t] + b_o$</code> 输出门得到的阈值向量</li><li><code>$h_t = o_t . tanh(c_t)$</code></li><li><del>从逻辑上来说，<code>$c_t$</code>就是个比较好的输出了，为什么还需要输出门去得到<code>$h_t$</code>呢？但是如果不得到<code>$h_t$</code>下个状态就没有隐藏状态了</del></li><li><code>$h_t$</code>是作为rnn单元的输出</li></ol></li></ol></li><li>训练算法</li><li>LSTM解决梯度消失问题的原因<ol><li>本质上来说，将原来的隐藏状态的更新公式从激活函数套激活函数中解放出来变成了加法的形式，从而避免了梯度连乘</li><li>普通rnn的更新形式：<code>$s_t = \sigma(UX_t+WS_{t-1})$</code>，LSTM的更新形式：<code>$C_t = i_t*g_t + f_t*C_{t-1}$</code></li><li><code>$\frac{\partial c_t}{\partial c_k} = f_t*\frac{\partial c_{t-1}}{\partial c_k}$</code></li></ol></li></ol><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><img src="http://owxv9mpb2.bkt.clouddn.com/gru.png" alt="gru"></p><ol><li>与LSTM两点不同<ol><li>将遗忘门、输出门和输入门合并为两个门：更新门和重置门</li><li>将两种状态合并为一种状态</li></ol></li><li>前向计算 <strong>为什么改成这样会有比较好的效果？？</strong><ol><li><code>$Z_t = \sigma(W_z [h_{t-1}, x_t])$</code></li><li><code>$r_t = \sigma(W_r [h_{t-1}, x_t])$</code></li><li><code>$h_t^- = tanh(W([r_t.h_{t-1}, x_t]))$</code></li><li><code>$h_t = (1 - Z_t).h_{t-1} + Z_t.h_t^-$</code></li></ol></li><li>个人理解<ol><li>其实GRU就是对以前信息是否加强做一个限制，而如果对以前的信息获取更多那么就存在了更加长期的记忆，如果少获取以前的信息那么就更多关注短期的记忆</li><li><code>$r_t$</code>如果是0的话，就是普通的RNN了，几乎对<code>$h_{t-1}$</code>不更多摄入；</li><li><code>$r_t$</code>如果比较大的话，相当于对以前的记忆进行更多地存储</li><li><code>$Z_t$</code>相当于以前的W和U矩阵</li><li>180312补充：以上的理解不算完全对，因为如果加上了激活函数照样爆炸</li></ol></li></ol>]]></content>
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rnn </tag>
            
            <tag> lstm </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
